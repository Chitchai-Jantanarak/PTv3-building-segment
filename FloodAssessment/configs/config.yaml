# MAE Pretraining Configuration
mae:
  stage: 0
  task: "mae_pretraining"
  data:
    root: "data/processed/train"
    voxel_size: 0.1
    batch_size: 1
    num_workers: 2
    max_points: 20000
  model:
    mask_ratio: 0.7
  training:
    epochs: 100
    lr: 0.001
    weight_decay: 0.05
    save_path: "checkpoints/ptv3_mae_encoder.pth"

# Structural Pipeline Configuration (Seg-B + Inpainting)
structural:
  stage: 1
  task: "structural_training"
  data:
    root: "/workspace/data/processed"
    voxel_size: 0.04
    batch_size: 4
  model:
    pretrained_encoder: "checkpoints/ptv3_mae_encoder.pth"
    freeze_encoder: true
  training:
    epochs: 50
    lr: 0.0005
    loss_weights:
      seg_b: 1.0
      inpaint: 0.1
    save_path: "checkpoints/ptv3_structural.pth"

# Semantic Pipeline Configuration (Seg-A + FEMA)
semantic:
  stage: 2
  task: "semantic_training"
  data:
    root: "/workspace/data/processed"
    voxel_size: 0.04
    batch_size: 4
  model:
    pretrained_encoder: "checkpoints/ptv3_structural.pth" # Initialize from structural? Or MAE? User said "Same MAE-pretrained PTv3 encoder initialization"
    # Actually User said: "Inputs: Geometry-regularized point cloud, Same MAE-pretrained PTv3 encoder initialization"
    # So we prefer MAE encoder here, not structural weights IF we want to strictly follow "Same initialization".
    # BUT usually refinement helps. I will stick to MAE encoder as requested explicitly.
    encoder_path: "checkpoints/ptv3_mae_encoder.pth"
  training:
    epochs: 50
    lr: 0.0005
    save_path: "checkpoints/ptv3_semantic.pth"
